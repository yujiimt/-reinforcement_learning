import os
import argparse
import random
from collections import deque
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.externals import joblib
import tensorflow as tf
from tensorflow.python import keras as k
import gym
from agent import FNAgent, Trainer, Observer, Experience


class PolicyGradientAgent(FNAgent):


    def __init__(self, epsilon, actions):
        super().__init__(epsilon, actions)
        self.estimate_probs = True
        self.scarler = None
        self._updater = None
    
    def save(self, model_path):
        super().save(model_path)
        joblib.dump(self.scaler, self.scaler_path(model_path))
    
    @classmethod
    def load(cls, env, model_path, epsilon=0.0001):
        agent = super().load(env, model_path, epsilon)
        agent.scaler = joblib.load(agent.scaler_path(model_path))
        return agent

    def scaler_path(self, model_path):
        fname, _ = os.path.splitext(model_path)
        fname += "_scaler.pkl"
        return fname
    
    def initialize(self, experiences, optimizer):
        self.scaler = StandardScaler()
        states = np.vstack([e.s for e in experiences])
        self.scaler.fit(states)

        feature_size = states.shape[1]
        self.model = k.models.Sequential([
            k.layers.Dense(10, activation = "relu", input_shape = (feature_size,)),
            k.layers.Dense(10, activation = "relu"),
            k.layers.Dense(len(self.actions), activation = "softmax")
        ])
        self.set_update(optimizer)
        self.initialized = True
        print("Done initialization . From now, begin training !")

    def set_update(self, optimizer):
        actions = tf.placeholder(shape = (None), dtype = "int32")
        rewards = tf.placeholder(shape = (None), dtype = "float32")
        one_hot_actions = tf.one_hot(actions, len(self.actions), axis=1)
        action_probs = self.model.output
        selected_action_probs = tf.reduce_sum(one_hot_actions * action_probs, axis = 1)
        
        clipped = tf.clip_by_value(selected_action_probs, 1e-10, 1.0)

        loss = -tf.log(clipped) * rewards
        loss = tf.reduce_mean(loss)

        updates = optimizer.get_updates(loss=loss, params = self.model.trainable_weights)
        
        self._updater = k.backend.function(
            inputs = [self.model.input,
                      actions, rewards],
            outputs = [loss],
            updates = updates)
    
    def estimate(self, s):
        normalized = self.scaler.transform(s)
        action_probs = self.model.predict(normalized)[0]
        return action_probs

    def update(self, states, actions, rewards):
        normalizeds = self.scaler.transform(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        self._updater([normalizeds, actions, rewards])

class CartPoleObserver(Observer):

    def transform(self, state):
        return np.array(state).reshape((1, -1))

class PolicyGradientTrainer(Trainer):

    def __init__(self, buffer_size = 1024, batch_size = 32, gamma = 0.9, report_interval = 10, log_dir=""):
        super().__init__(buffer_size, batch_size, gamma,
                            report_interval, log_dir)
        self._reward_scaler =  None
        self.d_experiences = deque(maxlen = buffer_size)
    
    def train(self, env, episode_count = 220, epsilon = 0.1, initial_count = -1, 
            render = False):
        actions = list(range(env.action_space.n))
        agent = PolicyGradientAgent(epsilon, actions)

        self.train_loop(env, agent, episode_count, initial_count, render)
        return agent

    def episode_begin(self, episode, agent):
        self.experiences = []
    
    def step(self, episode, step_count, agent, experience):
        if agent.initialized:
            agent.update(*self.make_batch())
    
    def make_batch(self):
        batch = random.sample(self.d_experiences, self.batch_size)
        states = np.vstack([e.s for e in batch])
        actions = [e.a for e in batch]
        rewards = [e.r for e in batch]
        rewards = np.array(rewards).reshape((-1, 1))
        rewards = self._reward_scaler.transform(rewards).flatten()
        return states, actions, rewards

    def begin_train(self, episode, agent):
        optimizer = k.optimizers.Adam(clipnorm = 1.0)
        agent.initialize(self.d_experiences, optimizer)
        self._reward_scaler = StandardScaler(with_mean = False)
        rewards = np.array([e.r] for e in self.d_experiences)
        self._reward_scaler.fit(rewards)

    def episode_end(self, episode, step_count, agent):
        rewards = [e.r for e in self.experiences]
        self.reward_log.append(sum(rewards))

        discounteds = []
        for t, r in enumerate(rewards):
            d_r = [_r * (self.gamma ** i) for i, _r in enumerate(rewards[t:])]
            d_r = sum(d_r)
            discounteds.append(d_r)

        for i, e in enumerate(self.experiences):
            s, a, r, n_s, d = e
            d_r = discounteds[i]
            d_e = Experience(s, a, d_r, n_s, d)
            self.d_experiences.append(d_e)

        if not self.training and len(self.d_experiences) == self.buffer_size:
            self.begin_train(i, agent)
            self.training = True
        
        if self.is_event(episode, self.report_interval):
            recent_rewards = self.reward_log[-self.report_interval:]
            self.logger.describe("reward", recent_rewards, episode=episode)


def main(play):
    env = CartPoleObserver(gym.make("CartPole-v0"))
    trainer = PolicyGradientTrainer()
    path = trainer.logger.path_of("policy_gradient_agent.h5")

    if play:
        agent = PolicyGradientAgent.load(env, path)
        agent.play(env)
    else:
        trained = trainer.train(env, episode_count=250)
        trainer.logger.plot("Rewards", trainer.reward_log, trainer.reward_interval)

        trained.save(path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = "PG Agent")
    parser.add_argument("--play", action = "store_true",
                        help = "play with trained model")
    
    args = parser.parse_args()
    main(args.play)